---
title: "JAMP Metabarcoding Pipeline"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: console
---

## Load packages and folder_rename function
```{r include=FALSE}
# Load packages
packages <- c('JAMP', 'here', 'tidyverse', 'vegan')
lapply(packages, library, character.only = T)
source("code/JAMP_folder_rename.R")

```

## Check all data made it
This is a very simple check to make sure that all reads have an R1 and R2 fastq file. If there is a missing file or the names do not match the ERROR column will read "ERROR" otherwise it says "fine". 
```{r}
# Make sure that all reads are paired, sudo check that no files were left behind
first_reads <- list.files(here("input_files", "_data"), pattern = ".*R1.fastq", full.names = TRUE)
second_reads <- list.files(here("input_files", "_data"), pattern = ".*R2.fastq", full.names = TRUE)
unmerged_file_names <- data.frame(first_reads, second_reads) 
unmerged_file_names$first_reads <- substr(unmerged_file_names$first_reads, 1, nchar(unmerged_file_names$first_reads)-9)
unmerged_file_names$second_reads <- substr(unmerged_file_names$second_reads, 1, nchar(unmerged_file_names$second_reads)-9)
unmerged_file_names$ERROR <- ifelse(unmerged_file_names$first_reads != unmerged_file_names$second_reads, "ERROR", "fine")
```

## Merge pair end reads
Two empty samples (CR_216_1_S45, CR_97_1_S29), and BA_1_S92 didn't merge successfully. 
BA_1_S92 was missing a sequence at line 22 of the R2 file. I completely removed this sequence from both files.
```{r}
# vsearch cannot auto pair R1 and R2? So used lapply to iterate over unmerged_file_names df.
lapply(unmerged_file_names, Merge_PE(file1=first_reads, file2=second_reads, exe= "vsearch"))
if (length(list.files(here("A_merge_PE", "_data"))) == 95) {
  print("Success!")
}
```

## Trim primers
fwhF2 and EPTDr2n

EPTDr2n ended with a Y degenerate base, which IDT was unable to produce. Instead, two primers were created, one ending with C (EPTDr2n_C) and the other ending with T (EPTDr2n_T).

All primers are sets of 4 primers with 0-3 N bases to create frame shifts.
```{r include=FALSE}
# import primer sequences (just COI primer sequence, not including universal tail)

primers <- read.csv(here("input_files", "fwhF2_EPTDr2n_primer_sequences.csv"))

#identify name of forward and reverse primer
f.primer <- "fwhF2_"
r.primer <- "EPTDr2n_"

#return TRUE or FALSE if each primer is Forward or Reverse
is_f <- grepl(f.primer, primers$Name)
is_r <- grepl(r.primer, primers$Name)

#create character string with forward and reverse primers
fwhF2 <- primers$Sequence[primers$Name=="fwhF2"]
EPTDr2n <- primers$Sequence[primers$Name=="EPTDr2n"]

primer_sequences <- cbind(fwhF2, EPTDr2n)

# trim primers 
merged_files <- list.files(here("A_merge_PE", "_data"), full.names = TRUE)

Cutadapt(files = merged_files, 
         forward = fwhF2,
         reverse = EPTDr2n,
         bothsides=T)

#by using "bothsides=T", forward or reverse primers are detected on both ends. This is not nessesary for fusion primers.

```


## get length distribution of trimmed sequences
```{r include=FALSE}

dir.create("_Length_distribution", path = "B_Cutadapt/_stats/_Length_distribution")

trimmed_files <- list.files(here("B_Cutadapt", "_data"), full.names = TRUE) # Create list of trimmed filenames

start_time <- Sys.time() # start timer to see how long the following for loop will take

# Loop over all trimmed files. Create length distribution pdf with name of the sample (cutoff file path and suffix)
for (file in trimmed_files) {
  Length_distribution(sequFile = file, out = paste("B_Cutadapt/_stats/_Length_distribution/", 
                                                    substr(file, nchar("/Users/cedarmackaness/gc/B_Cutadapt/_data/ "), 
                                                    nchar(file) - nchar("_PE_cut.fastq")), ".pdf", sep=""), fastq=TRUE)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

```


## Discard all sequences that are not an exact match for sequence length (142bp)
```{r}
#Filter trimmed reads to only match 142 bp
Cutadapt_files <- list.files("B_Cutadapt/_data", full.names = TRUE)
Minmax(files = Cutadapt_files,
       min=(142),
       max=(142))
JAMP_folder_rename(newname="minmax_haplotypes")
```


## Create two folders, one for ee value of 0.5 and one for ee value of 1.0
```{r}
haplotype_minmax_files <- list.files(here(recent_newname, "_data"), full.names = TRUE)

Max_ee(files = haplotype_minmax_files,
       max_ee=0.5)
JAMP_folder_rename(newname="maxee_haplotypes_maxee0_5")

Max_ee(files = haplotype_minmax_files,
       max_ee=1.0)
JAMP_folder_rename(newname="maxee_haplotypes_maxee1_0")
```


## Denoise with different alpha values and cluster

3 files seem to get duplicated with the duplicates being all zero's. The three files are BA_2_S61_PE.1, CR_226_1_S47_PE.1, and DIA_2_S88_PE.1.
```{r}
files_for_denoise_maxee0_5 <- list.files(here("maxee_haplotypes_maxee0_5_2023-07-05", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee0_5,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee0_5", "_alpha_", i, sep=""))
}

files_for_denoise_maxee1_0 <- list.files(here("maxee_haplotypes_maxee1_0_2023-07-05", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee1_0,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee1_0", "_alpha_", i, sep=""))
}
```


## Create dissimilarity matricies for mantel tests
For each ee filtering and alpha value denoising, calculate a dissimilarity matrix of sampling sites based on haplotype abundances.
Using the Cao index to account for potentially high beta diversity, and the unknown relationships between sampling and read counts for aquatic sampling of eDNA. SHOULD PROBABLY USE BINOMIAL METHOD AND CONVERT TO PRESENCE/ABSCENCE?
```{r}
for (folder in dir(pattern = "^denoised_haplotypes")) {
  #read in haplotype data, dropping metadata columns and the duplicated sites 
  #(2 sites end up duplicated somewhere in the denoising pipeline of JAMP)
  haplo_table <- paste0(folder, "/E_haplo_table.csv") %>% read_csv(col_names = TRUE, show_col_types = FALSE, 
    col_select = -c("sort", "haplotype", "OTU", "sequences", "CR_216_2_S46_PE.1", "DIA_1_S79_PE.1")) 
  
  haplo_table <- haplo_table[1:nrow(haplo_table) - 1,] #remove last row (site totals)
  
  haplo_table <- t(haplo_table) #transpose the distance matrix so haplotypes are columns and sites are rows.
  
  vegdist(haplo_table, method = "cao", binary = FALSE) %>% #dissimilarity matrix using Cao index.
    as.matrix() %>% as.data.frame() %>% 
      #write to file named dist_matrix.csv in appropriate folder.
      write_csv(file = paste0(folder, "/dist_matrix.csv"),col_names = FALSE) 
}
```


## Mantel tests
Compare each dissimilarity matrix to the default settings of ee = 1.0 and unoise_alpha = 5
```{r}
default_dist_matrix <- read_csv("denoised_haplotypes_maxee1_0_alpha_5_2023-07-05/dist_matrix.csv", 
                                col_names = FALSE, show_col_types = FALSE) 

for (folder in dir(pattern = "^denoised_haplotypes")) {
  dist_matrix <- paste0(folder, "/dist_matrix.csv") %>% read_csv(col_names = FALSE, show_col_types = FALSE) 
  
  cat(paste0("mantel test for: ", folder, "\n"), file = "mantel_test_results.txt", append = TRUE)
  
  mantel_result <- mantel(as.dist(default_dist_matrix), as.dist(dist_matrix), 
                          method = "pearson", permutations = 999)
  
  #save results for each comparison to file
  cat("standardized mantel statistic (r-value): ", mantel_result$statistic, "\np-value: ", mantel_result$signif, "\n\n", 
      file = "mantel_test_results.txt", append = TRUE) 
}
```


## Compare number of haplotypes within OTU's between different denoising methods
Compare how the number of observed OTU's change with each method.
Use OTU centroid sequences to compare the number of ESV's associated with each sequence according to different denoising methods. 
```{r}
haplo2OTU <- list.files(path = ".", pattern = ".*_haplo2OTU.csv", recursive = TRUE, full.names = TRUE) %>% grep("_data", ., value = TRUE)
haplo_seqs <- data.frame(method=character(0), sequence=character(0), ESV_count=double(0))

for (file in haplo2OTU) {
  file_data <- read_csv(file, col_select = c("ESV_count", "sequences"), show_col_types = FALSE)
  file_data$method <- substr(file, 23, nchar(file)-44)
  haplo_seqs <- bind_rows(haplo_seqs, file_data)
}

# number of OTU's based on each filtering method. 
# As expected the strictest filtering has the fewest OTU's (but the difference is small ~30 OTU's)
OTU_count_by_method <- aggregate(haplo_seqs$sequences, by = list(Method=haplo_seqs$method), FUN = NROW)

# number of ESV's based on each filtering method. Nearly 450 more OTU's in the least strict method vs. most strict.
ESV_count_by_method <- aggregate(haplo_seqs$ESV_count, by = list(Method=haplo_seqs$method), FUN = sum)

# Compare number of ESV's per OTU centroid for each method. 
# If the number differs from default settings of ee = 1.0 and alpha = 5 by more than three then add to data frame ESV_method_differences.

# Each column is a method. Each row is a centroid sequence. each value is the number of ESV's associated with each centroid.
sequence_ESVs_by_method <- spread(haplo_seqs, key = "method", value = "ESV_count")

ESV_method_differences <- filter_at(sequence_ESVs_by_method, vars(starts_with("maxee")), any_vars(. != maxee0_5_alpha_1))
```

