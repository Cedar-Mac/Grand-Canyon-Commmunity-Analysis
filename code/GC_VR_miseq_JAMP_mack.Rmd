---
title: "JAMP Metabarcoding Pipeline"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: console
---

## Load packages and folder_rename function
```{r include=FALSE}
# Load packages
packages <- c('JAMP', 'here', 'tidyverse', 'vegan')
lapply(packages, library, character.only = T)
source("code/JAMP_folder_rename.R")

```

## Check all data made it
This is a very simple check to make sure that all reads have an R1 and R2 fastq file. If there is a missing file or the names do not match the ERROR column will read "ERROR" otherwise it says "fine". 
```{r}
# Make sure that all reads are paired, sudo check that no files were left behind
first_reads <- list.files(here("input_files", "_data"), pattern = ".*R1.fastq", full.names = TRUE)
second_reads <- list.files(here("input_files", "_data"), pattern = ".*R2.fastq", full.names = TRUE)
unmerged_file_names <- data.frame(first_reads, second_reads) 
unmerged_file_names$first_reads <- substr(unmerged_file_names$first_reads, 1, nchar(unmerged_file_names$first_reads)-9)
unmerged_file_names$second_reads <- substr(unmerged_file_names$second_reads, 1, nchar(unmerged_file_names$second_reads)-9)
unmerged_file_names$ERROR <- ifelse(unmerged_file_names$first_reads != unmerged_file_names$second_reads, "ERROR", "fine")
```

## Merge pair end reads
```{r}
# vsearch cannot auto pair R1 and R2? So used lapply to iterate over unmerged_file_names df.
lapply(unmerged_file_names, Merge_PE(file1=first_reads, file2=second_reads, exe= "vsearch"))
if (length(list.files(here("A_merge_PE", "_data"))) == 95) {
  print("Success!")
}

# Two empty samples (CR_216_1_S45, CR_97_1_S29), and BA_1_S92 didn't merge successfully
```

## Trim primers
fwhF2 and EPTDr2n

EPTDr2n ended with a Y degenerate base, which IDT was unable to produce. Instead, two primers were created, one ending with C (EPTDr2n_C) and the other ending with T (EPTDr2n_T).

All primers are sets of 4 primers with 0-3 N bases to create frame shifts.
```{r include=FALSE}

# import primer sequences (just COI primer sequence, not including universal tail)

primers <- read.csv(here("input_files", "fwhF2_EPTDr2n_primer_sequences.csv"))

#identify name of forward and reverse primer
f.primer <- "fwhF2_"
r.primer <- "EPTDr2n_"

#return TRUE or FALSE if each primer is Forward or Reverse
is_f <- grepl(f.primer, primers$Name)
is_r <- grepl(r.primer, primers$Name)

#create character string with forward and reverse primers
fwhF2 <- primers$Sequence[primers$Name=="fwhF2"]
EPTDr2n <- primers$Sequence[primers$Name=="EPTDr2n"]

primer_sequences <- cbind(fwhF2,EPTDr2n)

# trim primers 
merged_files <- list.files(here("A_merge_PE", "_data"), full.names = TRUE)

Cutadapt(files = merged_files, 
         forward = fwhF2,
         reverse = EPTDr2n,
         bothsides=T)

#by using "bothsides=T", forward or reverse primers are detected on both ends. This is not nessesary for fusion primers.

```


## get length distribution of trimmed sequences
```{r include=FALSE}

dir.create("_Length_distribution", path = "B_Cutadapt/_stats/_Length_distribution")

trimmed_files <- list.files(here("B_Cutadapt", "_data"), full.names = TRUE) # Create list of trimmed filenames

start_time <- Sys.time() # start timer to see how long the following for loop will take

# Loop over all trimmed files. Create length distribution pdf with name of the sample (cutoff file path and suffix)
for (file in trimmed_files) {
  Length_distribution(sequFile = file, out = paste("B_Cutadapt/_stats/_Length_distribution/", 
                                                    substr(file, nchar("/Users/cedarmackaness/gc/B_Cutadapt/_data/ "), 
                                                    nchar(file) - nchar("_PE_cut.fastq")), ".pdf", sep=""), fastq=TRUE)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

```


## Discard all sequences that are not an exact match for sequence length (142bp)
```{r}
#Filter trimmed reads to only match 142 bp
Cutadapt_files <- list.files("B_Cutadapt/_data", full.names = TRUE)
Minmax(files = Cutadapt_files,
       min=(142),
       max=(142))
JAMP_folder_rename(newname="minmax_haplotypes")
```


## Create two folders, one for ee value of 0.5 and one for ee value of 1.0
```{r}
haplotype_minmax_files <- list.files(here(recent_newname, "_data"), full.names = TRUE)

Max_ee(files = haplotype_minmax_files,
       max_ee=0.5)
JAMP_folder_rename(newname="maxee_haplotypes_maxee0_5")

Max_ee(files = haplotype_minmax_files,
       max_ee=1.0)
JAMP_folder_rename(newname="maxee_haplotypes_maxee1_0")
```


## Denoise with different alpha values and cluster
3 files seem to get duplicated with the duplicates being all zero's. The three files are BA_2_S61_PE.1, CR_226_1_S47_PE.1, and DIA_2_S88_PE.1.
```{r}
files_for_denoise_maxee0_5 <- list.files(here("maxee_haplotypes_maxee0_5_2023-07-04", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee0_5,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee0_5", "_alpha_", i, sep=""))
}

files_for_denoise_maxee1_0 <- list.files(here("maxee_haplotypes_maxee1_0_2023-07-04", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee1_0,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee1_0", "_alpha_", i, sep=""))
}
```


## Create dissimilarity matricies for mantel tests
For each ee filtering and alpha value denoising, calculate a dissimilarity matrix of sampling sites based on haplotype abundances.
Using the Cao index to account for potentially high beta diversity, and the unknown relationships between sampling and read counts for aquatic sampling of eDNA. SHOULD PROBABLY USE BINOMIAL METHOD AND CONVERT TO PRESENCE/ABSCENCE?
```{r}
for (folder in dir(pattern = "^denoised_haplotypes")) {
  #read in haplotype data, dropping metadata columns and the duplicated sites (3 sites end up duplicated somewhere in the denoising pipeline of JAMP)
  haplo_table <- paste0(folder, "/E_haplo_table.csv") %>% read_csv(col_names = TRUE, show_col_types = FALSE, 
    col_select = -c("sort", "haplotype", "OTU", "sequences", "BA_2_S61_PE.1", "CR_226_1_S47_PE.1", "DIA_2_S88_PE.1")) 
  
  haplo_table <- haplo_table[1:nrow(haplo_table) - 1,] #remove last row (site totals)
  
  haplo_table <- t(haplo_table) #transpose the distance matrix so haplotypes are columns and sites are rows.
  
  vegdist(haplo_table, method = "cao", binary = FALSE) %>% #dissimilarity matrix using Cao index.
    as.matrix() %>% as.data.frame() %>% 
      #write to file named dist_matrix.csv in appropriate folder.
      write_csv(file = paste0(folder, "/dist_matrix.csv"),col_names = FALSE) 
}
```


## Mantel tests
Compare each dissimilarity matrix to the default settings of ee = 1.0 and unoise_alpha = 5
```{r}
default_dist_matrix <- read_csv("denoised_haplotypes_maxee1_0_alpha_5_2023-07-04/dist_matrix.csv", 
                                col_names = FALSE, show_col_types = FALSE) 

for (folder in dir(pattern = "^denoised_haplotypes")) {
  dist_matrix <- paste0(folder, "/dist_matrix.csv") %>% read_csv(col_names = FALSE, show_col_types = FALSE) 
  
  cat(paste0("mantel test for: ", folder, "\n"), file = "mantel_test_results.txt", append = TRUE)
  
  mantel_result <- mantel(as.dist(default_dist_matrix), as.dist(dist_matrix), 
                          method = "pearson", permutations = 999)
  
  #save results for each comparison to file
  cat("mantel statistic (r-value): ", mantel_result$statistic, "\np-value: ", mantel_result$signif, "\n\n", 
      file = "mantel_test_results.txt", append = TRUE) 
}
```


## Default OTUs
Cluster sequences into OTUs using VSearch.
This OTU clustering is not subset to the lowest sequencing depth, something that is available with USearch but not with VSearch. At the moment I am going to cluster without subsetting, but plan to figure out how to subset in the future (11/19/21).
```{r include=FALSE}

# Take files from maxee_haplotypes_maxee1_0_2023-07-04, which were filtered at an EE of 1
Max_ee_files <- list.files("maxee_haplotypes_maxee1_0_2023-07-04/_data", full.names=T)

#cluster OTUs based on Max_ee files with a filter of 0.01%. This means that any sequence greater than 0.01% abundance in the total dataset will be included as an OTU
Cluster_otus(files= Max_ee_files,
             filter=0.01,
             otu_radius_pct=3)

#create new OTU heatmap with better color grading
OTU_heatmap(file = "F_Cluster_otus/5_OTU_table_0.01.csv",
            out = "GC_VR_OTU_Heatmap_0.01.pdf",
            abundance = TRUE,
            col = c("blue3", "white"))

```
