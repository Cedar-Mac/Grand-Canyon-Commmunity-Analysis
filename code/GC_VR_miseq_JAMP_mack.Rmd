---
title: "JAMP Metabarcoding Pipeline"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: console
---

## Load packages and folder_rename function
```{r include=FALSE}
# Load packages
packages <- c('JAMP', 'here', 'tidyverse', 'vegan', 'Biostrings')
lapply(packages, library, character.only = T)
source("code/JAMP_folder_rename.R")
```

## Check all data made it
This is a very simple check to make sure that all reads have an R1 and R2 fastq file. If there is a missing file or the names do not match the ERROR column will read "ERROR" otherwise it says "fine". 
First and second reads are the forward and reverse reads.
The ERROR column in the unmerged_file_names will say ERROR if names do not match or there is an unequal number of R1 and R2 reads. Could always count the number of ERROR occurances, but a quick visual scan is all I did.

```{r}
# Make sure that all reads are paired, sudo check that no files were left behind
first_reads <- list.files(here("input_files", "_data"), pattern = ".*R1.fastq", full.names = TRUE)
second_reads <- list.files(here("input_files", "_data"), pattern = ".*R2.fastq", full.names = TRUE)
unmerged_file_names <- data.frame(first_reads, second_reads) 

# Remove the _R1.fastq suffix and _R2.fastq suffix to compare names and verify that all rows have matching names
unmerged_file_names$first_reads <- substr(unmerged_file_names$first_reads, 1, nchar(unmerged_file_names$first_reads)-9)
unmerged_file_names$second_reads <- substr(unmerged_file_names$second_reads, 1, nchar(unmerged_file_names$second_reads)-9)
unmerged_file_names$ERROR <- ifelse(unmerged_file_names$first_reads != unmerged_file_names$second_reads, "ERROR", "fine")
```

## Merge pair end reads
Merging the paired reads takes the forward and reverse strands and their quality scores and essentially cross-checks them to create a single single corrected strand with filled N's and new quality scores. Merge_PE from the JAMP pipeline is the function to do this. Loop apply the function over all fastq files.

Two empty samples (CR_216_1_S45, CR_97_1_S29), and BA_1_S92 didn't merge successfully. 
BA_1_S92 was missing a sequence at line 22 of the R2 file. I completely removed this sequence from both files.
```{r}
# vsearch cannot auto pair R1 and R2? So used lapply to iterate over unmerged_file_names df.
lapply(unmerged_file_names, Merge_PE(file1=first_reads, file2=second_reads, exe= "vsearch"))

# Make sure all reads merged successfully (there should be 95 resulting files).
if (length(list.files(here("A_merge_PE", "_data"))) == 95) {
  print("Success!")
}
```

## Trim primers
fwhF2 and EPTDr2n

EPTDr2n ended with a Y degenerate base, which IDT was unable to produce. Instead, two primers were created, one ending with C (EPTDr2n_C) and the other ending with T (EPTDr2n_T).

All primers are sets of 4 primers with 0-3 N bases to create frame shifts.
```{r include=FALSE}
# Import primer sequences (just COI primer sequence, not including universal tail)

primers <- read.csv(here("input_files", "fwhF2_EPTDr2n_primer_sequences.csv"))

# Identify name of forward and reverse primer
f.primer <- "fwhF2_"
r.primer <- "EPTDr2n_"

# Return TRUE or FALSE if each primer is Forward or Reverse
is_f <- grepl(f.primer, primers$Name)
is_r <- grepl(r.primer, primers$Name)

# Create character string with forward and reverse primers
fwhF2 <- primers$Sequence[primers$Name=="fwhF2"]
EPTDr2n <- primers$Sequence[primers$Name=="EPTDr2n"]

primer_sequences <- cbind(fwhF2, EPTDr2n)

# Trim primers 
merged_files <- list.files(here("A_merge_PE", "_data"), full.names = TRUE)

Cutadapt(files = merged_files, 
         forward = fwhF2,
         reverse = EPTDr2n,
         bothsides=T)

# By using "bothsides=T", forward or reverse primers are detected on both ends. This is not nessesary for fusion primers.

```


## get length distribution of trimmed sequences
This is a good quality check. Target sequences should be 142 bp plus or minus a few bases for indels.

```{r include=FALSE}

dir.create("_Length_distribution", path = "B_Cutadapt/_stats/_Length_distribution")

trimmed_files <- list.files(here("B_Cutadapt", "_data"), full.names = TRUE) # Create list of trimmed filenames

start_time <- Sys.time() # start timer to see how long the following for loop will take

# Loop over all trimmed files. Create length distribution pdf with name of the sample (cutoff file path and suffix)
for (file in trimmed_files) {
  Length_distribution(sequFile = file, 
                      out = paste("B_Cutadapt/_stats/_Length_distribution/", 
                                  substr(file, start = nchar("/Users/cedarmackaness/gc/B_Cutadapt/_data/ "), 
                                         nchar(file) - nchar("_PE_cut.fastq")), ".pdf", sep=""), fastq=TRUE)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

```


## Discard all sequences that are not an exact match for sequence length (142bp)
```{r}
#Filter trimmed reads to only match 142 bp
Cutadapt_files <- list.files("B_Cutadapt/_data", full.names = TRUE)
Minmax(files = Cutadapt_files,
       min=(142),
       max=(142))
JAMP_folder_rename(newname="minmax_haplotypes")
```


## Create two folders, one for ee value of 0.5 and one for ee value of 1.0
At this point we still have fastq files as the output of the Minmax filter. I now create two folders for filtered sequences based on their quality scores. One is filtered at an expected error rate of 0.5 errors per 100 bases, the other at 1 error per 100 bases. 
```{r}
haplotype_minmax_files <- list.files(here(recent_newname, "_data"), full.names = TRUE)

Max_ee(files = haplotype_minmax_files,
       max_ee=0.5)
JAMP_folder_rename(newname="maxee_haplotypes_maxee0_5")

Max_ee(files = haplotype_minmax_files,
       max_ee=1.0)
JAMP_folder_rename(newname="maxee_haplotypes_maxee1_0")
```


## Denoise with different alpha values and different expected error cut-offs and cluster
The alpha value determines at what point a similar sequence x is determined to be a mistaken version of sequence y. A lower alpha value means that the distance between sequences has to be large, otherwise sequence x will be considered the same as sequence y. A higher alpha value will be more relaxed and result in more "unique" sequences at the risk of identifying sequences as unique that were actually mistaken versions.

The JAMP Denoise function performs several tasks. First chimera sequences (CO1 genes that appear outside of invertebrate mitochondrial DNA?) are removed. Sequences are then denoised (supposedly erroneous sequences are merged with the correct sequence). and finally sequences are clustered into OTU's.

3 files seem to get duplicated with the duplicates being all zero's. The three files are BA_2_S61_PE.1, CR_226_1_S47_PE.1, and DIA_2_S88_PE.1. Issue resolved, an extra row in the files was the issue.
```{r}
files_for_denoise_maxee0_5 <- list.files(here("maxee_haplotypes_maxee0_5_2023-07-05", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee0_5,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee0_5", "_alpha_", i, sep=""))
}

files_for_denoise_maxee1_0 <- list.files(here("maxee_haplotypes_maxee1_0_2023-07-05", "_data"), full.names = TRUE)
for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee1_0,
        minsize = 10,
        minrelsize = 0.001,
        unoise_alpha = i)
  JAMP_folder_rename(newname=paste("denoised_haplotypes_maxee1_0", "_alpha_", i, sep=""))
}
```


## Create dissimilarity matricies for mantel tests
For each ee filtering and alpha value denoising, calculate a dissimilarity matrix of sampling sites based on haplotype abundances.
Using the Cao index to account for potentially high beta diversity, and the unknown relationships between sampling and read counts for aquatic sampling of eDNA. SHOULD PROBABLY USE BINOMIAL METHOD AND CONVERT TO PRESENCE/ABSCENCE?
```{r}
for (folder in dir(pattern = "^denoised_haplotypes")) {
  #read in haplotype data, dropping metadata columns and the duplicated sites 
  #(2 sites end up duplicated somewhere in the denoising pipeline of JAMP)
  haplo_table <- paste0(folder, "/E_haplo_table.csv") %>% read_csv(col_names = TRUE, show_col_types = FALSE, 
    col_select = -c("sort", "haplotype", "OTU", "sequences", "CR_216_2_S46_PE.1", "DIA_1_S79_PE.1")) 
  
  haplo_table <- haplo_table[1:nrow(haplo_table) - 1,] #remove last row (site totals)
  
  haplo_table <- t(haplo_table) #transpose the distance matrix so haplotypes are columns and sites are rows.
  
  vegdist(haplo_table, method = "cao", binary = FALSE) %>% #dissimilarity matrix using Cao index.
    as.matrix() %>% as.data.frame() %>% 
      #write to file named dist_matrix.csv in appropriate folder.
      write_csv(file = paste0(folder, "/dist_matrix.csv"),col_names = FALSE) 
}
```


## Mantel tests
Compare each dissimilarity matrix to the default settings of ee = 1.0 and unoise_alpha = 5
```{r}
default_dist_matrix <- read_csv("denoised_haplotypes_maxee1_0_alpha_5_2023-07-05/dist_matrix.csv", 
                                col_names = FALSE, show_col_types = FALSE) 

for (folder in dir(pattern = "^denoised_haplotypes")) {
  dist_matrix <- paste0(folder, "/dist_matrix.csv") %>% read_csv(col_names = FALSE, show_col_types = FALSE) 
  
  cat(paste0("mantel test for: ", folder, "\n"), file = "mantel_test_results.txt", append = TRUE)
  
  mantel_result <- mantel(as.dist(default_dist_matrix), as.dist(dist_matrix), 
                          method = "pearson", permutations = 999)
  
  #save results for each comparison to file
  cat("standardized mantel statistic (r-value): ", mantel_result$statistic, "\np-value: ", mantel_result$signif, "\n\n", 
      file = "mantel_test_results.txt", append = TRUE) 
}
```


## Compare number of haplotypes within OTU's between different denoising methods
Compare how the number of observed OTU's change with each method.
Use OTU centroid sequences to compare the number of ESV's associated with each sequence according to different denoising methods. 
```{r}
haplo2OTU <- list.files(path = ".", pattern = ".*_haplo2OTU.csv", recursive = TRUE, full.names = TRUE) %>% grep("_data", ., value = TRUE)
otu_seqs <- data.frame(method=character(0), sequences=character(0), ESV_count=double(0))

for (file in haplo2OTU) {
  file_data <- read_csv(file, col_select = c("ESV_count", "sequences"), show_col_types = FALSE)
  file_data$method <- substr(file, 23, nchar(file)-44)
  otu_seqs <- bind_rows(otu_seqs, file_data)
}

# number of OTU's based on each filtering method. 
# As expected the strictest filtering has the fewest OTU's (but the difference is small ~30 OTU's)
OTU_count_by_method <- aggregate(otu_seqs$sequences, by = list(Method=otu_seqs$method), FUN = NROW)
OTU_count_by_method$maxee <- parse_number(unlist(strsplit(OTU_count_by_method$Method, "alpha")))[seq(1, 32, 2)] #create new column with maxee
OTU_count_by_method[OTU_count_by_method == 0] <- 0.5
OTU_count_by_method$alpha <- parse_number(unlist(strsplit(OTU_count_by_method$Method, "alpha")))[seq(2, 32, 2)] #create new column with alpha
# Plot results
OTU_count_by_method %>% 
  ggplot((aes(x=alpha, y=x, group=factor(maxee), color=factor(maxee)))) +
    geom_line() +
    scale_color_discrete() +
    ggtitle("Number of OTU's based on denoising parameters") +
    ylab("Number of OTU's")

# number of ESV's based on each filtering method. Nearly 450 more OTU's in the least strict method vs. most strict.
ESV_count_by_method <- aggregate(otu_seqs$ESV_count, by = list(Method=otu_seqs$method), FUN = sum)
ESV_count_by_method$maxee <- parse_number(unlist(strsplit(ESV_count_by_method$Method, "alpha")))[seq(1, 32, 2)] #create new column with maxee
ESV_count_by_method[ESV_count_by_method == 0] <- 0.5
ESV_count_by_method$alpha <- parse_number(unlist(strsplit(ESV_count_by_method$Method, "alpha")))[seq(2, 32, 2)] #create new column with alpha

# Plot results
ESV_count_by_method %>% 
  ggplot((aes(x=alpha, y=x, group=factor(maxee), color=factor(maxee)))) +
    geom_line() +
    scale_color_discrete() +
    ggtitle("Number of ESV's based on denoising parameters") +
    ylab("Number of ESV's")
  
# Compare number of ESV's per OTU centroid for each method. 
# Each column is a method. Each row is a centroid sequence. each value is the number of ESV's associated with each centroid.
sequence_ESVs_by_method <- spread(otu_seqs, key = "method", value = "ESV_count")

#These are centroid sequences where at least one method had a different number of ESV's than the rest.
ESV_method_differences <- filter_at(sequence_ESVs_by_method, vars(starts_with("maxee")), any_vars(. != maxee0_5_alpha_1))
```
As expected the number of OTU's and the number of ESV's increases as the alpha value is relaxed. The line for ee=0.5 remains lower than the less strict filtering of ee=1.0. 
The number of observed OTU's flattens out quickly as the alpha value increases becoming flat after an alpha of 4. 
The number of ESV's increases until an alpha value of 8. 
This suggests there may be some rare ESV's that are being discarded at lower alpha values, or that these ESV's are actually sequencing errors and were rightfully discarded. 
Regardless, from a species conservation standpoint, it appears that an alpha value of 3 is perfectly acceptable, and relaxing the alpha value will not significantly increase the number of OTU's observed.
When considering species haplotypes looking at phylogeny and population genetics it may be worth considering a more relaxed alpha value of 8. However, this has the risk of including erroneous sequences.

## NMS of the number of ESV's per OTU in method space
```{r}
sequence_ESVs_by_method[is.na(sequence_ESVs_by_method)] <- 0
ESV_MDS_in <- t(sequence_ESVs_by_method)[-1,]
mode(ESV_MDS_in) <- "numeric"

ESV_MDS_out <- metaMDS(ESV_MDS_in, autotransform = FALSE,
          distance = "bray",
          k = 3,
          maxit = 999, 
          trymax = 500,
          wascores = TRUE)
stressplot(ESV_MDS_out)
plot(ESV_MDS_out, "sites")
orditorp(ESV_MDS_out, "sites")
```

## Compare mean entropy scores
A few different ideas here:
  1. Corrected entropy score a la Antich:
      for each method calculate the mean entropy for each codon position in a sequence -> 
      average across all sequences for that method ->
      calculate final entropy score of the sequence by weighted average of the average mean entropy score.
      formula -- sum(i=3) entropy(i) * 3 / (entropy(1) + entropy(2) + entropy(3))
  2. First position entropy score:
      For each method take the mean entropy for a single codon position (likely position 1) ->
      average across all sequences for that method, use that as final entropy value for the method
  3. Entropy ratio a la Weitmer:
      For each method find the average of the mean entropy scores for position 2 and 3 across sequences ->
      calculate the ratio of average mean entropy score of position 2 and 3.
  4. Information content:
      Information content takes the maximum possible entropy value and subtracts the observed entropy value.
  5. Something complicated:
      Something to do with the entropy scores of individual sequences. 
      Maybe flag method if any individual sequence has an outrageous entropy score.
      Maybe develop a filter for haplotype sequences based on the entropy score.
```{r}
# Going with option 3, entropy ratio.
haplo_seqs <- list.files(path = ".", pattern = ".*_haplo_sequ_by_OTU.txt", recursive = TRUE, full.names = TRUE) %>% 
  grep("_data", ., value = TRUE)

entropy_ratio_by_method <- data.frame(method=character(0), entropy_ratio=double(0))

start_time <- Sys.time()

for (file in haplo_seqs) {
  haplo_seq_StringSet <- readDNAStringSet(file, format = "fasta", use.names = TRUE)
  entropy_scores <- taxreturn::codon_entropy(haplo_seq_StringSet, genetic_code = "SGC4", codon_filter = TRUE, method = "ML")
    #method "ML" is the maximum likelihood. Calculates Shannon Entropy (H) using frequencies of bases.
    #maximum entropy is 1.3863 nats (ln) for a completely random distribution of base frequency (expected for pos3).
    #genetic code "SGC4" is the invertebrate mitochondrial reference genome.
  pos1_mean_entropy <- lapply(1:length(entropy_scores), function(x) entropy_scores[[x]][[1]]) %>% unlist() %>% mean()
  pos2_info <- log(4) - pos1_mean_entropy
  pos2_mean_entropy <- lapply(1:length(entropy_scores), function(x) entropy_scores[[x]][[2]]) %>% unlist() %>% mean()
  pos2_info <- log(4) - pos2_mean_entropy
  pos3_mean_entropy <- lapply(1:length(entropy_scores), function(x) entropy_scores[[x]][[3]]) %>% unlist() %>% mean()
  pos3_info <- log(4) - pos3_mean_entropy
  entropy_ratio <- pos2_mean_entropy / pos3_mean_entropy
  entropy_ratio_by_method <- add_row(entropy_ratio_by_method, method = substr(file, 23, nchar(file) - 52), entropy_ratio = entropy_ratio)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

# Add column for maxee and alpha value
entropy_ratio_by_method$maxee <- parse_number(unlist(strsplit(entropy_ratio_by_method$method, "alpha")))[seq(1, 32, 2)] 
entropy_ratio_by_method$maxee[entropy_ratio_by_method$maxee == 0] <- 0.5
entropy_ratio_by_method$alpha <- parse_number(unlist(strsplit(entropy_ratio_by_method$method, "alpha")))[seq(2, 32, 2)] 

# Graph entropy ratio vs. alpha value for each ee.
entropy_ratio_by_method %>% 
  ggplot((aes(x=alpha, y=entropy_ratio, group=factor(maxee), color=factor(maxee)))) +
    geom_line() +
    scale_color_discrete() +
    ggtitle("Codon Entropy Ratio from Taxreturn") +
    ylab("Entropy Ratio (P2/P3)")
```

71-72 sequences are being removed because they contain stop codons. These are clearly not true haplotypes.
Additionally one sequence is always being reverse complimented, this is probably not supposed to be here after filtering and denoising. My guess is chimeral DNA that somehow got past the denoising process of JAMP.
Current ratio is greater than 1 which would mean that position 3 has a lower entropy value/is more predictable. That goes against standard pattern of codon entropy. Potentially this is an issue with alignment and the codon_entropy function. Alternatively, since the entropy is empirical and based on observed counts in the sequences, it may be that the CO1 gene sequence being used is too short (142 bp). This could throw off base counts at each codon position. I believe the alignment issue could be very likely because the entropy value for position 1 would fit for position 3, and the entropy value of position 3 would fit with what we know about position 2 being highly conserved. I will test for this with multiple sequence alignment to a reference CO1 gene.

## Verify entropy scores myself
I ran a Blastx search on a random sequence. It appears a +2 shift in the reading frame is necessary to align with protein synthesis, therefore the first base in our sequences are actually the 3rd. I'm not sure if the taxreturn::codon_entropy function is taking that into account. If it is not taking the shift into account then the output is actually much closer to what is expected. We will verify here.
```{r}
haplo_seqs <- list.files(path = ".", pattern = ".*_haplo_sequ_by_OTU.txt", recursive = TRUE, full.names = TRUE) %>% 
  grep("_data", ., value = TRUE)

my_entropy_ratios <- data.frame(method=character(0), entropy_ratio=double(0))

for (file in haplo_seqs) {
  # read in file
  haplo_seq_StringSet <- readDNAStringSet(file, format = "fasta", use.names = TRUE)
 
   # create probability table of each base at each position.
  position_probability_matrix <- consensusMatrix(haplo_seq_StringSet, as.prob = TRUE, shift = 2, baseOnly = TRUE)
  
  # motif logo of the consensus matrix.
  ggseqlogo::ggseqlogo(position_probability_matrix)
 
   # calculate entropy at each position using Shannon entropy formula with natural logarithm (to match the nats unit above)
  position_entropy_vector <- apply(position_probability_matrix, 2, function(prob) -sum(prob * log(prob), na.rm = TRUE))
 
   # reshape so there are three columns, one for each codon position. Fill the first two zero's as NA and rename columns.
  codon_position_entropy_matrix <- matrix(position_entropy_vector, ncol = 3, byrow = TRUE)
  codon_position_entropy_matrix[codon_position_entropy_matrix==0] <- NA
  colnames(codon_position_entropy_matrix) <- c("pos_1", "pos_2", "pos_3")
  
  # calculate mean entropy of each codon position.
  pos_1_mean <- colMeans(codon_position_entropy_matrix, na.rm = TRUE)[1]
  pos_2_mean <- colMeans(codon_position_entropy_matrix, na.rm = TRUE)[2]
  pos_3_mean <- colMeans(codon_position_entropy_matrix, na.rm = TRUE)[3]
  
  # ratio of most conserved position (position 2) and the most variable position (position 3)
  ratio <- pos_2_mean / pos_3_mean
  
  # add the method results to the data frame and repeat.
  my_entropy_ratios <- add_row(my_entropy_ratios, method = substr(file, 23, nchar(file) - 52), entropy_ratio = ratio)
}

# Add column for maxee and alpha value
my_entropy_ratios$maxee <- parse_number(unlist(strsplit(my_entropy_ratios$method, "alpha")))[seq(1, 32, 2)] 
my_entropy_ratios$maxee[my_entropy_ratios$maxee == 0] <- 0.5
my_entropy_ratios$alpha <- parse_number(unlist(strsplit(my_entropy_ratios$method, "alpha")))[seq(2, 32, 2)] 

# Graph entropy ratio vs. alpha value for each ee.
my_entropy_ratios %>% 
  ggplot((aes(x=alpha, y=entropy_ratio, group=factor(maxee), color=factor(maxee)))) +
    geom_line() +
    scale_color_discrete() +
    ggtitle("Codon Entropy Ratio Comparing Between Sequences") +
    ylab("Entropy Ratio (P2/P3)")
```

There seems to be an issue where the codon_entropy is calculating the entropy within a sequence, i.e. counting all the occurrences of 'A' at position 1 of each codon. Above, I calculated the entropy at each position in the sequence by comparing haplotype sequences. I'll try it the AlexPiper method below.

```{r}
haplo_seqs <- list.files(path = ".", pattern = ".*_haplo_sequ_by_OTU.txt", recursive = TRUE, full.names = TRUE) %>% 
  grep("_data", ., value = TRUE)

codon_entropy_table <- data.frame(pos1=double(0), pos2=double(0), pos3=double(0))
codon_entropy_ratios <- data.frame(method=character(0), entropy_ratio=double(0))

start_time <- Sys.time()

for (file in haplo_seqs) {
  # read in file
  haplo_seq_StringSet <- readDNAStringSet(file, format = "fasta", use.names = TRUE)
  
  
  for (sequence in seq_along(haplo_seq_StringSet)) {
    # Turn into vector of characters to select all bases in codon position 1, 2 and 3.
    shifted_seq <- haplo_seq_StringSet[sequence] %>% as.character() %>% 
    paste("--", ., sep = '') %>% 
    str_split("") %>% 
    unlist() 
    
    # Take every third base starting at positions 1, 2, and 3 to get the list of bases at each position
    pos_1_bases <- shifted_seq[seq(1, length(shifted_seq), 3)]
    pos_2_bases <- shifted_seq[seq(2, length(shifted_seq), 3)]
    pos_3_bases <- shifted_seq[seq(3, length(shifted_seq), 3)]
    
    # Formula for Shannon H Entropy. find the probability of each base with table(pos)[base] / 47. Plug into Entropy formula
    pos_1_entropy <- -sum((table(pos_1_bases)["A"] / 47) * log((table(pos_1_bases)["A"] / 47)) + 
        (table(pos_1_bases)["C"] / 47) * log((table(pos_1_bases)["C"] / 47)) + 
          (table(pos_1_bases)["G"] / 47) * log((table(pos_1_bases)["G"] / 47)) +
          (table(pos_1_bases)["T"] / 47) * log((table(pos_1_bases)["T"] / 47)))

    pos_2_entropy <- -sum((table(pos_2_bases)["A"] / 47) * log((table(pos_2_bases)["A"] / 47)) + 
        (table(pos_2_bases)["C"] / 47) * log((table(pos_2_bases)["C"] / 47)) + 
          (table(pos_2_bases)["G"] / 47) * log((table(pos_2_bases)["G"] / 47)) +
          (table(pos_2_bases)["T"] / 47) * log((table(pos_2_bases)["T"] / 47)))
    
    pos_3_entropy <- -sum((table(pos_3_bases)["A"] / 48) * log((table(pos_3_bases)["A"] / 48)) + 
        (table(pos_3_bases)["C"] / 48) * log((table(pos_3_bases)["C"] / 48)) + 
          (table(pos_3_bases)["G"] / 48) * log((table(pos_3_bases)["G"] / 48)) +
          (table(pos_3_bases)["T"] / 48) * log((table(pos_3_bases)["T"] / 48)))
    codon_entropy_table <- add_row(codon_entropy_table, 
                                   pos1 = pos_1_entropy,
                                   pos2 = pos_2_entropy,
                                   pos3 = pos_3_entropy)
    }

  
  codon_entropy_ratio = mean(codon_entropy_table$pos2) / mean(codon_entropy_table$pos3, na.rm = TRUE)
  
  codon_entropy_ratios <- add_row(codon_entropy_ratios, 
                                  method = substr(file, 23, nchar(file) - 52), 
                                  entropy_ratio = codon_entropy_ratio)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

# Add column for maxee and alpha value
codon_entropy_ratios$maxee <- parse_number(unlist(strsplit(codon_entropy_ratios$method, "alpha")))[seq(1, 32, 2)] 
codon_entropy_ratios$maxee[codon_entropy_ratios$maxee == 0] <- 0.5
codon_entropy_ratios$alpha <- parse_number(unlist(strsplit(codon_entropy_ratios$method, "alpha")))[seq(2, 32, 2)] 

# Graph entropy ratio vs. alpha value for each ee.
codon_entropy_ratios %>% 
  ggplot((aes(x=alpha, y=entropy_ratio, group=factor(maxee), color=factor(maxee)))) +
    geom_line() +
    scale_color_discrete() +
    ggtitle("Codon Entropy Ratio Comparing Within Sequence") +
    ylab("Entropy Ratio (P2/P3)")
```

The entropy ratio of codon position 2 and codon position 3 should be less than 1 (codon position 3 is more variable, and therefore has a higher entropy score). At low alpha values we are being very strict, this should remove more variability from position 2 (lower entropy score) while position 3 stays relatively unchanged (already variable). This should result in small ratios at low alpha values. As denoising becomes less strict there will be increased variability at codon 2 compared to codon 3 resulting in higher ratios at higher alpha values. For some reason I do not see that in either manual method. That is the trend supported by Weitmer et al. so I must be doing something wrong? I have double-checked the formulas, so I'm not really sure where this could be happening.