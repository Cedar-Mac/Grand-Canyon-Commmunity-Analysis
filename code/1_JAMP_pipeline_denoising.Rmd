---
title: "JAMP Pipeline Denoising"
output:
  pdf_document: default
  html_notebook: default
editor_options:
  chunk_output_type: console
---

## Load packages and folder_rename function
```{r include=FALSE}
# Load packages
packages <- c('JAMP', 'tidyverse', 'vegan')
lapply(packages, library, character.only = T)
source("code/JAMP_folder_rename.R")
```

## Check all data made it
This is a very simple check to make sure that all reads have an R1 and R2 fastq file. If there is a missing file or the names do not match the ERROR column will read "ERROR" otherwise it says "fine". 
First and second reads are the forward and reverse reads.
The ERROR column in the unmerged_file_names will say ERROR if names do not match or there is an unequal number of R1 and R2 reads. Could always count the number of ERROR occurances, but a quick visual scan is all I did.

```{r check file names}
# Make sure that all reads are paired, sudo check that no files were left behind
first_reads <- list.files(
  path = here("input_files", "_data"), 
  pattern = ".*R1.fastq", 
  full.names = TRUE)
second_reads <- list.files(
  path = here("input_files", "_data"), 
  pattern = ".*R2.fastq", 
  full.names = TRUE)
unmerged_file_names <- data.frame(first_reads, second_reads) 

# Remove the _R1.fastq suffix and _R2.fastq suffix and verify names match
unmerged_file_names$first_reads <- substring(
  text = unmerged_file_names$first_reads, 
  first = 1, 
  last = nchar(unmerged_file_names$first_reads)-9)

unmerged_file_names$second_reads <- substring(
  text = unmerged_file_names$second_reads, 
  first = 1, 
  last = nchar(unmerged_file_names$second_reads)-9)

unmerged_file_names$ERROR <- ifelse(
  test = unmerged_file_names$first_reads != unmerged_file_names$second_reads, 
  yes = "ERROR", 
  no = "fine")
```

## Merge pair end reads
Merging the paired reads takes the forward and reverse strands and their quality scores and essentially cross-checks them to create a single single corrected strand with filled N's and new quality scores. Merge_PE from the JAMP pipeline is the function to do this. Loop apply the function over all fastq files.

Two empty samples (CR_216_1_S45, CR_97_1_S29), and BA_1_S92 didn't merge successfully. 
BA_1_S92 was missing a sequence at line 22 of the R2 file. I completely removed this sequence from both files and the merge is now successful.
```{r merge pairs}
# Use lapply to iterate over unmerged_file_names df.
unmerged_file_names %>%
  lapply(Merge_PE(file1 = first_reads, file2 = second_reads, exe = "vsearch"))

# Make sure all reads merged successfully (there should be 95 resulting files).
if (length(list.files(here("A_merge_PE", "_data"))) == 95) {
  print("Success!")
}
```

## Trim primers
fwhF2 and EPTDr2n

EPTDr2n ended with a Y degenerate base, which IDT was unable to produce. Instead, two primers were created, one ending with C (EPTDr2n_C) and the other ending with T (EPTDr2n_T).

All primers are sets of 4 primers with 0-3 N bases to create frame shifts.
```{r trim primers, include=FALSE}
# Import primer sequences (just COI primer sequence, not including universal tail)
primers_file <- "input_files/fwhF2_EPTDr2n_primer_sequences.csv"
primers <- read.csv(primers_file)

# Identify name of forward and reverse primer
f.primer <- "fwhF2_"
r.primer <- "EPTDr2n_"

# Return TRUE or FALSE if each primer is Forward or Reverse
is_f <- grepl(f.primer, primers$Name)
is_r <- grepl(r.primer, primers$Name)

# Create character string with forward and reverse primers
fwhF2 <- primers$Sequence[primers$Name=="fwhF2"]
EPTDr2n <- primers$Sequence[primers$Name=="EPTDr2n"]

primer_sequences <- cbind(fwhF2, EPTDr2n)

# Trim primers 
merged_files <- list.files(here("A_merge_PE", "_data"), full.names = TRUE)

Cutadapt(files = merged_files, 
         forward = fwhF2,
         reverse = EPTDr2n,
         bothsides=T)

# By using "bothsides=T", forward or reverse primers are detected on both ends. This is not nessesary for fusion primers.

```


## get length distribution of trimmed sequences
This is a good quality check. Target sequences should be 142 bp plus or minus a few indels.

```{r length distributions include=FALSE}

dir.create("_Length_distribution", path = "B_Cutadapt/_stats/_Length_distribution")

trimmed_files <- list.files(here("B_Cutadapt", "_data"), full.names = TRUE) # Create list of trimmed filenames

start_time <- Sys.time() # start timer to see how long the following for loop will take

# Loop over all trimmed files. Create length distribution pdf with name of the sample (cutoff file path and suffix)
for (file in trimmed_files) {
  Length_distribution(
    sequFile = file, 
    out = paste("B_Cutadapt/_stats/_Length_distribution/", 
          substr(file, start = nchar("/Users/cedarmackaness/gc/B_Cutadapt/_data/ "), 
          nchar(file) - nchar("_PE_cut.fastq")), ".pdf", sep=""), fastq=TRUE)
}

end_time <- Sys.time() # end timer once for loop is completed

end_time - start_time

```


## Discard all sequences that are not an exact match for sequence length (142bp)
```{r 142 trim}
#Filter trimmed reads to only match 142 bp
Cutadapt_files <- list.files("B_Cutadapt/_data", full.names = TRUE)
Minmax(files = Cutadapt_files,
       min=(142),
       max=(142))

JAMP_folder_rename(newname = "minmax_haplotypes")
```


## Create two folders, one for ee value of 0.5 and one for ee value of 1.0
At this point we still have fastq files as the output of the Minmax filter. I now create two folders for filtered sequences based on their quality scores. One is filtered at an expected error rate of 0.5 errors per 100 bases, the other at 1 error per 100 bases. 
```{r ee filtering}
min_max_path = paste(recent_newname, "/_data", sep = "")
haplotype_minmax_files <- list.files(min_max_path, full.names = TRUE)

Max_ee(files = haplotype_minmax_files,
       max_ee=0.5)
JAMP_folder_rename(newname = "maxee_haplotypes_maxee0_5")

Max_ee(files = haplotype_minmax_files,
       max_ee=1.0)
JAMP_folder_rename(newname = "maxee_haplotypes_maxee1_0")
```


## Denoise with different alpha values and different expected error cut-offs and cluster
The alpha value determines at what point a similar sequence x is determined to be a mistaken version of sequence y. A lower alpha value means that the distance between sequences has to be large, otherwise sequence x will be considered the same as sequence y. A higher alpha value will be more relaxed and result in more "unique" sequences at the risk of identifying sequences as unique that were actually mistaken versions.

The JAMP Denoise function performs several tasks. First, sequences are denoised (supposedly erroneous sequences are merged with the correct sequence). and finally sequences are clustered into OTU's. Then, chimera sequences (CO1 genes that appear outside of invertebrate mitochondrial DNA?) are removed. Finally, abundance filtering is applied based on minsize and minrelsize parameters. There are some other abundance filtering parameters with the defaults used such as withinOTU and OTUmin.

I am curious about the withinOTU parameter. It seems to discard a highly variable number of sequences (2% to >70%). I believe this is removing all haplotypes with less than 5% relative abundance within each OTU.

3 files seem to get duplicated with the duplicates being all zero's. The three files are BA_2_S61_PE.1, CR_226_1_S47_PE.1, and DIA_2_S88_PE.1. Issue resolved, an extra row in the files was the issue.
```{r denoise}
# denoise files filtered at EE = 0.5
ee0_5_files <- "maxee_haplotypes_maxee0_5_2023-08-25/_data"
files_for_denoise_maxee0_5 <- list.files(ee0_5_files, full.names = TRUE)

for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee0_5,
          # remove sequences with less than 10 occurrences within a site
          minsize = 10,
          # remove sequences with less than 0.01% relative abundance for each site
          minrelsize = 0.001,
          # set alpha parameter (see for loop)
          unoise_alpha = i)
  
  JAMP_folder_rename(
    newname = paste("denoised_haplotypes_maxee0_5", "_alpha_", i, sep=""))
}

# denoise files filtered at EE = 1.0
ee1_0_files <- "maxee_haplotypes_maxee1_0_2023-08-25/_data"
files_for_denoise_maxee1_0 <- list.files(ee1_0_files, full.names = TRUE)

for (i in c("1", "3", "5", "7", "9", "11", "13", "15")) {
  Denoise(files = files_for_denoise_maxee1_0,
          # remove sequences with less than 10 occurrences within a site
          minsize = 10,
          # remove sequences with less than 0.1% relative abundance for each site
          minrelsize = 0.001,
          # set alpha parameter (see for loop)
          unoise_alpha = i)
  
  JAMP_folder_rename(
    newname = paste("denoised_haplotypes_maxee1_0", "_alpha_", i, sep=""))
}
```


